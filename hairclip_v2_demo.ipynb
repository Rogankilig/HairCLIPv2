{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T08:16:09.395381Z",
     "start_time": "2024-04-14T08:16:09.335410900Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'kornia'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_15052\\3078096173.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtorchvision\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mtransforms\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mscripts\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mEmbedding\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mEmbedding\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 8\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mscripts\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtext_proxy\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mTextProxy\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      9\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mscripts\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mref_proxy\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mRefProxy\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mscripts\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msketch_proxy\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mSketchProxy\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\PycharmProjects\\HairCLIPv2\\scripts\\text_proxy.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mcriteria\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mclip_loss\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mCLIPLoss\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mAugCLIPLoss\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      3\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mface_alignment\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mutils\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mimage_utils\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mprocess_display_input\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfunctional\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mF\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\PycharmProjects\\HairCLIPv2\\criteria\\clip_loss.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mclip\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtorchvision\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mtransforms\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 4\u001B[1;33m \u001B[1;32mimport\u001B[0m \u001B[0mkornia\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0maugmentation\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mK\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      5\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfunctional\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mF\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'kornia'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(0)\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from scripts.Embedding import Embedding\n",
    "from scripts.text_proxy import TextProxy\n",
    "from scripts.ref_proxy import RefProxy\n",
    "from scripts.sketch_proxy import SketchProxy\n",
    "from scripts.bald_proxy import BaldProxy\n",
    "from scripts.color_proxy import ColorProxy\n",
    "from scripts.feature_blending import hairstyle_feature_blending\n",
    "from utils.seg_utils import vis_seg\n",
    "from utils.mask_ui import painting_mask\n",
    "from utils.image_utils import display_image_list, process_display_input\n",
    "from utils.model_utils import load_base_models\n",
    "from utils.options import Options\n",
    "333666"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-14T08:16:09.355401700Z"
    }
   },
   "outputs": [],
   "source": [
    "opts = Options().parse(jupyter=True)\n",
    "src_name = '168125'# source image name you want to edit\n",
    "\n",
    "image_transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
    "g_ema, mean_latent_code, seg = load_base_models(opts)\n",
    "ii2s = Embedding(opts, g_ema, mean_latent_code[0,0])\n",
    "if not os.path.isfile(os.path.join(opts.src_latent_dir, f\"{src_name}.npz\")):\n",
    "    inverted_latent_w_plus, inverted_latent_F = ii2s.invert_image_in_FS(image_path=f'{opts.src_img_dir}/{src_name}.jpg')\n",
    "    save_latent_path = os.path.join(opts.src_latent_dir, f'{src_name}.npz')\n",
    "    np.savez(save_latent_path, latent_in=inverted_latent_w_plus.detach().cpu().numpy(),\n",
    "                latent_F=inverted_latent_F.detach().cpu().numpy())\n",
    "src_latent = torch.from_numpy(np.load(f'{opts.src_latent_dir}/{src_name}.npz')['latent_in']).cuda()\n",
    "src_feature = torch.from_numpy(np.load(f'{opts.src_latent_dir}/{src_name}.npz')['latent_F']).cuda()\n",
    "src_image = image_transform(Image.open(f'{opts.src_img_dir}/{src_name}.jpg').convert('RGB')).unsqueeze(0).cuda()\n",
    "input_mask = torch.argmax(seg(src_image)[1], dim=1).long().clone().detach()\n",
    "\n",
    "bald_proxy = BaldProxy(g_ema, opts.bald_path)\n",
    "text_proxy = TextProxy(opts, g_ema, seg, mean_latent_code)\n",
    "ref_proxy = RefProxy(opts, g_ema, seg, ii2s)\n",
    "sketch_proxy = SketchProxy(g_ema, mean_latent_code, opts.sketch_path)\n",
    "color_proxy = ColorProxy(opts, g_ema, seg)\n",
    "\n",
    "edited_hairstyle_img = src_image\n",
    "def hairstyle_editing(global_cond=None, local_sketch=False, paint_the_mask=False, \\\n",
    "                      src_latent=src_latent, src_feature=src_feature, input_mask=input_mask, src_image=src_image, \\\n",
    "                        latent_global=None, latent_local=None, latent_bald=None, local_blending_mask=None, painted_mask=None):\n",
    "    if paint_the_mask:\n",
    "        modified_mask = painting_mask(input_mask)\n",
    "        input_mask = torch.from_numpy(modified_mask).unsqueeze(0).cuda().long().clone().detach()\n",
    "        vis_modified_mask = vis_seg(modified_mask)\n",
    "        display_image_list([src_image, vis_modified_mask])\n",
    "        painted_mask = input_mask\n",
    "\n",
    "    if local_sketch:\n",
    "        latent_local, local_blending_mask, visual_local_list = sketch_proxy(input_mask)\n",
    "        display_image_list(visual_local_list)\n",
    "        \n",
    "    if global_cond is not None:\n",
    "        assert isinstance(global_cond, str)\n",
    "        latent_bald, visual_bald_list = bald_proxy(src_latent)\n",
    "        display_image_list(visual_bald_list)\n",
    "\n",
    "        if global_cond.endswith('.jpg') or global_cond.endswith('.png'):\n",
    "            latent_global, visual_global_list = ref_proxy(global_cond, src_image, painted_mask=painted_mask)\n",
    "        else:\n",
    "            latent_global, visual_global_list = text_proxy(global_cond, src_image, from_mean=True, painted_mask=painted_mask)\n",
    "        display_image_list(visual_global_list)\n",
    "\n",
    "    src_feature, edited_hairstyle_img = hairstyle_feature_blending(g_ema, seg, src_latent, src_feature, input_mask, latent_bald=latent_bald,\\\n",
    "                                                latent_global=latent_global, latent_local=latent_local, local_blending_mask=local_blending_mask)\n",
    "    return src_feature, edited_hairstyle_img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hairstyle Editing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-14T08:16:09.365394600Z"
    }
   },
   "outputs": [],
   "source": [
    "#global_cond: e.g. 'bowl cut hairstyle' for text_mode; '058728.jpg' for ref_mode\n",
    "src_feature, edited_hairstyle_img = hairstyle_editing(global_cond='bowl cut hairstyle', local_sketch=False, paint_the_mask=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-14T08:16:09.365394600Z"
    }
   },
   "outputs": [],
   "source": [
    "display_image_list([process_display_input(src_image), process_display_input(edited_hairstyle_img)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Color Editing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-14T08:16:09.365394600Z"
    }
   },
   "outputs": [],
   "source": [
    "color_cond = '108157.jpg' #e.g. 'red hair' for text_mode; '108157.jpg' for ref_mode; (220,220,220) for RGB value mode\n",
    "\n",
    "visual_color_list, visual_final_list = color_proxy(color_cond, edited_hairstyle_img, src_latent, src_feature)\n",
    "display_image_list(visual_color_list)\n",
    "display_image_list(visual_final_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-14T08:16:09.375390300Z"
    }
   },
   "outputs": [],
   "source": [
    "display_image_list([src_image, visual_final_list[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-14T08:16:09.375390300Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pytorch_gpu",
   "language": "python",
   "display_name": "Python(pytorch_gpu)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "42999d0f7672f45d22c7af74163c9fa321bb4063269a2058f7a180f2a01bc77e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
